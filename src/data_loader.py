import pandas as pd
import numpy as np
from sklearn.model_selection import GroupShuffleSplit
import os

class DataLoader:
    def __init__(self, filepath):
        self.filepath = filepath
        self.raw_df = None
        self.processed_df = None

    def load_data(self):
        """Loads valid columns from the dataset."""
        print(f"Loading data from {self.filepath}...")
        # Columns based on inspection: Vitals, Labs, Demographics, SepsisLabel, Patient_ID
        self.raw_df = pd.read_csv(self.filepath)
        print(f"Data loaded: {self.raw_df.shape}")
        return self.raw_df

    def preprocess(self):
        """
        1. Sort by Patient and Time
        2. Impute Missing Values (Forward Fill -> Median Fill)
        3. Feature Engineering (Lags, Deltas, Rolling)
        4. Drop rows that couldn't be engineered (e.g., first few hours) or fill them
        """
        print("Preprocessing data...")
        df = self.raw_df.copy()

        # Ensure sorted by Patient and Time (assuming 'Hour' or implicit ordering)
        # Using implicit ordering as 'Hour' exists 
        df.sort_values(by=['Patient_ID', 'Hour'], inplace=True)

        # 1. Imputation
        # Forward fill within patient groups to propagate last known vitals
        print("Imputing missing values...")
        # Groupby apply is slow, using ffill() on sorted is faster if we respect boundaries
        # Safe approach: Groupby ffill
        # NOTE: groupby().ffill() drops the grouping column! We must restore it.
        patient_ids = df['Patient_ID']
        df = df.groupby('Patient_ID').ffill()
        df['Patient_ID'] = patient_ids
        
        # Remaining NaNs (start of records) filled with global median
        df = df.fillna(df.median())

        # 2. Feature Engineering
        print("Engineering Time-Series Features (Lags, Deltas, Rolling)...")
        
        # Key Vitals to engineer
        vitals = ['HR', 'MAP', 'SBP', 'O2Sat', 'Temp', 'Resp']
        # Check if MAP exists, else calculate (approx) or skip
        if 'MAP' not in df.columns and 'SBP' in df.columns and 'DBP' in df.columns:
             df['MAP'] = (df['SBP'] + 2*df['DBP']) / 3
        
        existing_vitals = [v for v in vitals if v in df.columns]

        for col in existing_vitals:
            # Shift 1 (Previous Hour)
            df[f'{col}_Lag1'] = df.groupby('Patient_ID')[col].shift(1)
            
            # Delta (Current - Prev)
            df[f'{col}_Delta'] = df[col] - df[f'{col}_Lag1']
            
            # Rolling Mean (past 6 hours)
            # Optimized: groupby().rolling() returns MultiIndex, we reset level 0 to match df index
            rolled = df.groupby('Patient_ID')[col].rolling(window=6, min_periods=1).mean()
            df[f'{col}_RollMean6h'] = rolled.reset_index(level=0, drop=True)

        # Handle NaNs generated by Lagging (first row of patient)
        # We assume Lag_1 = Current for the first row (Delta = 0) as discussed in the Cold Start plan
        for col in existing_vitals:
            df[f'{col}_Lag1'] = df[f'{col}_Lag1'].fillna(df[col])
            df[f'{col}_Delta'] = df[f'{col}_Delta'].fillna(0)
            df[f'{col}_RollMean6h'] = df[f'{col}_RollMean6h'].fillna(df[col])

        self.processed_df = df
        print(f"Preprocessing complete. Features: {df.shape[1]}")
        return self.processed_df

    def split_data(self, test_size=0.2, val_size=0.1):
        """Splits data by Patient_ID to avoid leakage."""
        print("Splitting data (Train/Val/Test) by Patient_ID...")
        
        X = self.processed_df.drop(columns=['SepsisLabel', 'Patient_ID'])
        y = self.processed_df['SepsisLabel']
        groups = self.processed_df['Patient_ID']

        # Split 1: Train+Val vs Test
        splitter_test = GroupShuffleSplit(test_size=test_size, n_splits=1, random_state=42)
        train_val_idx, test_idx = next(splitter_test.split(X, y, groups))
        
        X_train_val = X.iloc[train_val_idx]
        y_train_val = y.iloc[train_val_idx]
        groups_train_val = groups.iloc[train_val_idx]
        
        X_test = X.iloc[test_idx]
        y_test = y.iloc[test_idx]

        # Split 2: Train vs Val (from Train+Val)
        # Adjust val_size relative to remaining data
        # raw val_size=0.1. If test=0.2, remaining is 0.8. 0.1/0.8 = 0.125
        relative_val_size = val_size / (1 - test_size)
        splitter_val = GroupShuffleSplit(test_size=relative_val_size, n_splits=1, random_state=42)
        train_idx, val_idx = next(splitter_val.split(X_train_val, y_train_val, groups_train_val))

        X_train = X_train_val.iloc[train_idx]
        y_train = y_train_val.iloc[train_idx]
        
        X_val = X_train_val.iloc[val_idx]
        y_val = y_train_val.iloc[val_idx]
        
        print(f"Train: {X_train.shape[0]} rows")
        print(f"Val:   {X_val.shape[0]} rows")
        print(f"Test:  {X_test.shape[0]} rows")
        
        return X_train, y_train, X_val, y_val, X_test, y_test

if __name__ == "__main__":
    # Test the loader
    loader = DataLoader(r'c:\Users\Admin\Desktop\mimic-iv-clinical-database-demo-2.2\Dataset.csv')
    loader.load_data()
    loader.preprocess()
    loader.split_data()
